# autogenerated by metadata-generator
# Sourced from Terraform module code.
# See TF Blueprint Metadata standard: https://docs.google.com/document/d/1uMhNixV5RsH3s21pniLZpQDkSLS2cABh2lo1fkq2hMY/edit

apiVersion: blueprints.cloud.google.com/v1alpha1
kind: BlueprintMetadata
metadata:
  name: terraform-google-dataflow-job
  source:
    repo: https://github.com/GoogleCloudPlatform/terraform-google-dataflow-job # Example source repo
    sourceType: git
spec:
  info:
    title: Dataflow Job
    version: 1.0.0 # Example version
    actuationTool:
      type: Terraform
      versions:
      - '>= 1.3'
    description:
      tagline: Deploys a classic or Flex Template Dataflow job with an optional dedicated service account.
      detailed: This module handles the deployment of a Google Cloud Dataflow job. It supports both classic and Flex Templates. It can also create a dedicated service account for the job and configure the necessary IAM permissions on the project and required GCS buckets, following the principle of least privilege.
    icon: icon.png # Example icon path
  requirements:
    services:
    - dataflow.googleapis.com
    - iam.googleapis.com
    - cloudresourcemanager.googleapis.com
    - storage.googleapis.com
    roles:
    - level: Project
      roles:
      - roles/dataflow.developer
      - roles/iam.serviceAccountAdmin # Required if create_service_account is true
      - roles/resourcemanager.projectIamAdmin # Required if create_service_account is true
    - level: Bucket
      roles:
      - roles/storage.admin # Required if create_service_account is true to set IAM on buckets
  customization:
    variables:
    - name: name
      description: A unique name for the Dataflow job. Must be unique within the project and region. If null, no job is created.
      type: string
      default: null
      required: true
    - name: project_id
      description: The Google Cloud project ID to deploy the Dataflow job into. If null, no job is created.
      type: string
      default: null
      required: true
    - name: region
      description: The GCP region to run the Dataflow job in.
      type: string
      default: us-central1
      required: false
    - name: use_flex_template
      description: Set to true to create a job from a Flex Template. If false, a classic template job is created.
      type: boolean
      default: false
      required: false
    - name: template_gcs_path
      description: The GCS path to the classic Dataflow template. Required if use_flex_template is false.
      type: string
      default: null
      required: false
    - name: temp_gcs_location
      description: A GCS path for Dataflow to stage temporary job files, e.g., 'gs://your-bucket/temp'. Required for classic template jobs.
      type: string
      default: null
      required: false
    - name: container_spec_gcs_path
      description: The GCS path to the Flex Template JSON spec file. Required if use_flex_template is true.
      type: string
      default: null
      required: false
    - name: parameters
      description: A map of key-value parameters to pass to the Dataflow job.
      type: object
      default: {}
      required: false
    - name: zone
      description: The GCP zone to run the Dataflow job in. If null, the service will pick a zone in the specified region. Applies only to classic template jobs.
      type: string
      default: null
      required: false
    - name: on_delete
      description: Action to take when the Terraform resource is destroyed. Can be 'cancel' or 'drain'.
      type: string
      default: cancel
      required: false
    - name: max_workers
      description: The maximum number of workers to use for the Dataflow job. If not set, the service default will be used.
      type: number
      default: null
      required: false
    - name: machine_type
      description: The machine type to use for the Dataflow workers (e.g., n1-standard-1). If not set, the service default will be used.
      type: string
      default: null
      required: false
    - name: network
      description: The VPC network to which the Dataflow workers will be attached. Should be of the form 'projects/PROJECT/global/networks/NETWORK'. If not specified, the default network will be used.
      type: string
      default: null
      required: false
    - name: subnetwork
      description: The VPC subnetwork to which the Dataflow workers will be attached. Should be of the form 'regions/REGION/subnetworks/SUBNETWORK'.
      type: string
      default: null
      required: false
    - name: ip_configuration
      description: The IP configuration for the Dataflow workers. Can be 'WORKER_IP_PUBLIC' or 'WORKER_IP_PRIVATE'.
      type: string
      default: null
      required: false
    - name: create_service_account
      description: If true, a new service account is created for the Dataflow job. If false, the job uses the service account specified in 'service_account_email', or the default Compute Engine service account if that is empty.
      type: boolean
      default: false
      required: false
    - name: service_account_name
      description: The name for the service account to be created (the part of the email before the '@'). Required if 'create_service_account' is true.
      type: string
      default: null
      required: false
    - name: service_account_email
      description: The email of the service account for the Dataflow job to run as. Ignored if 'create_service_account' is true. If empty, the default Compute Engine service account is used.
      type: string
      default: null
      required: false
    - name: labels
      description: A map of key-value labels to apply to the Dataflow job.
      type: object
      default: {}
      required: false
    - name: additional_experiments
      description: A list of additional experiments to enable for the Dataflow job.
      type: array
      items:
        type: string
      default: []
      required: false
    - name: enable_streaming_engine
      description: Enable Streaming Engine for streaming jobs.
      type: boolean
      default: null
      required: false
    - name: skip_wait_on_job_termination
      description: If set to true, Terraform will not wait for the Flex Template job to finish before considering the resource created. Applies only to flex template jobs.
      type: boolean
      default: false
      required: false
    variableGroups:
    - name: Job Configuration
      description: Core settings for the Dataflow job.
      variables:
      - name
      - project_id
      - region
      - use_flex_template
      - zone
      - on_delete
    - name: Template Configuration
      description: Settings for classic and Flex Templates.
      variables:
      - template_gcs_path
      - temp_gcs_location
      - container_spec_gcs_path
      - parameters
    - name: Worker Configuration
      description: Configuration for the Dataflow worker pool.
      variables:
      - max_workers
      - machine_type
      - network
      - subnetwork
      - ip_configuration
    - name: Service Account Configuration
      description: Settings for the service account used to run the job.
      variables:
      - create_service_account
      - service_account_name
      - service_account_email
    - name: Advanced Configuration
      description: Additional and advanced job settings.
      variables:
      - labels
      - additional_experiments
      - enable_streaming_engine
      - skip_wait_on_job_termination
    outputs:
    - name: job_id
      description: The unique ID of the created Dataflow job.
    - name: job_name
      description: The name of the Dataflow job.
    - name: job_state
      description: The current state of the Dataflow job.
    - name: job_type
      description: The type of the Dataflow job.
    - name: created_service_account_email
      description: The email of the service account created by this module for the Dataflow job, if any.
