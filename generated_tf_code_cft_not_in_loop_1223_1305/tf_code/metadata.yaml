#
# Copyright 2024 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
apiVersion: blueprints.cloud.google.com/v1alpha1
kind: TerraformBlueprint
metadata:
  name: terraform-google-dataflow-job
  source:
    type: git
    git:
      repo: https://github.com/GoogleCloudPlatform/terraform-google-dataflow-job.git
      dir: /
  version: 1.0.0
spec:
  info:
    title: Dataflow Job
    actuationTool:
      type: Terraform
      version: '>= 1.3'
    descriptions:
      tagline: Deploys a Google Cloud Dataflow job from a classic or Flex Template.
      detailed: This module deploys a Google Cloud Dataflow job using the unified `google_dataflow_job` resource. It supports creating jobs from both classic templates and modern Flex Templates, selectable via a boolean flag. The module provides comprehensive configuration options for job parameters, worker settings, networking, and security.
    labels:
      gcp: ''
      dataflow: ''
      serverless: ''
    examples: []
  content:
    variables:
    - name: additional_experiments
      description: List of experiments to enable for the Dataflow job.
      type: array
      default: []
      required: false
    - name: container_spec_gcs_path
      description: The GCS path to the Flex Template container spec file. Required when `use_flex_template` is true.
      type: string
      default: null
      required: false
    - name: ip_configuration
      description: The IP configuration for the workers. Valid values are `WORKER_IP_PUBLIC` or `WORKER_IP_PRIVATE`.
      type: string
      default: null
      required: false
    - name: kms_key_name
      description: The Cloud KMS key to use for this job. The key must be provided in the full format of `projects/<project>/locations/<location>/keyRings/<keyring>/cryptoKeys/<key>`.
      type: string
      default: null
      required: false
    - name: labels
      description: A map of key-value pairs to assign to the Dataflow job.
      type: object
      default: {}
      required: false
    - name: machine_type
      description: The machine type to use for the Dataflow workers (e.g., `n1-standard-1`).
      type: string
      default: null
      required: false
    - name: max_workers
      description: The maximum number of workers to use for an autoscaling job. This should not be used with `num_workers`.
      type: number
      default: null
      required: false
    - name: name
      description: A unique name for the Dataflow job.
      type: string
      default: dataflow-job
      required: false
    - name: network
      description: The VPC network to which the workers will be deployed. Must be in the same project and region as the job.
      type: string
      default: null
      required: false
    - name: num_workers
      description: The initial number of workers to use for a fixed-size job. This should not be used with `max_workers`. This is passed as a `numWorkers` parameter to the job.
      type: number
      default: null
      required: false
    - name: on_delete
      description: Defines the behavior of the streaming job when the resource is destroyed. Valid values are `drain` and `cancel`. `drain` is recommended for streaming jobs to prevent data loss.
      type: string
      default: cancel
      required: false
    - name: parameters
      description: A map of key-value parameters to pass to the Dataflow job.
      type: object
      default: {}
      required: false
    - name: project_id
      description: The GCP project ID where the Dataflow job will be created. If not specified, the provider project is used.
      type: string
      default: null
      required: false
    - name: region
      description: The region in which the Dataflow job will be executed.
      type: string
      default: us-central1
      required: false
    - name: service_account_email
      description: The service account email to run the Dataflow job as. If not specified, the default Compute Engine service account will be used. This service account must have the 'roles/dataflow.worker' role and any other permissions required by the job.
      type: string
      default: null
      required: false
    - name: subnetwork
      description: The VPC subnetwork to which the workers will be deployed. Must be in the same project and region as the job. The subnetwork must be provided in the format `regions/REGION/subnetworks/SUBNETWORK`.
      type: string
      default: null
      required: false
    - name: temp_gcs_location
      description: A GCS path for Dataflow to stage temporary job files. This is required for all template jobs and MUST be overridden with a path to a GCS bucket you own. The bucket must exist.
      type: string
      default: gs://your-gcs-bucket-for-temp/temp
      required: true
    - name: template_gcs_path
      description: The GCS path to the classic Dataflow template. Required when `use_flex_template` is false.
      type: string
      default: null
      required: false
    - name: use_flex_template
      description: Set to true to create a Dataflow job from a Flex Template. If false, a classic template job is created.
      type: boolean
      default: false
      required: false
    - name: zone
      description: The zone in which the Dataflow job will be executed. If not specified, the service will pick a default zone in the region.
      type: string
      default: null
      required: false
    variableGroups:
    - name: Job Configuration
      description: Basic settings for the Dataflow job, including its name and location.
      variables:
      - name
      - project_id
      - region
      - zone
      - labels
    - name: Template Settings
      description: Configure the type of template (Classic or Flex) and its parameters. The temporary GCS location is required for job staging.
      variables:
      - use_flex_template
      - template_gcs_path
      - container_spec_gcs_path
      - temp_gcs_location
      - parameters
    - name: Worker Configuration
      description: Define the compute resources for the Dataflow workers.
      variables:
      - service_account_email
      - machine_type
      - num_workers
      - max_workers
    - name: Networking
      description: Specify VPC network and subnetwork for the Dataflow workers.
      variables:
      - network
      - subnetwork
      - ip_configuration
    - name: Advanced Settings
      description: Advanced job settings for lifecycle, security, and experimentation.
      variables:
      - on_delete
      - kms_key_name
      - additional_experiments
    outputs:
    - name: job_id
      description: The unique ID of the created Dataflow job.
    - name: job_name
      description: The name of the created Dataflow job.
    - name: job_state
      description: The current state of the Dataflow job.
    - name: job_type
      description: The type of the Dataflow job (e.g., JOB_TYPE_STREAMING, JOB_TYPE_BATCH).
  requirements:
    roles:
    - level: Project
      roles:
      - roles/dataflow.developer
      - roles/storage.objectAdmin
      - roles/iam.serviceAccountUser
    services:
    - dataflow.googleapis.com
    - compute.googleapis.com
    - storage.googleapis.com
    - cloudkms.googleapis.com
